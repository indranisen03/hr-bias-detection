# Fairness in HR Performance Reviews: Automated Bias Detection System

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue)](https://www.python.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![RAG](https://img.shields.io/badge/RAG-Enabled-orange)](https://www.anthropic.com/)
[![Status](https://img.shields.io/badge/Status-Research%20Prototype-yellow)](https://github.com)

An AI-powered system for detecting and mitigating gender bias in HR performance reviews using RAG (Retrieval-Augmented Generation) and agentic workflows.

## ğŸ¯ Problem Statement

HR performance reviews systematically exhibit gender bias:
- **Women** receive "communal" language (supportive, collaborative, helpful)
- **Men** receive "agentic" language (leader, strategic, drives results)

This language differential correlates with:
- Lower performance ratings for women
- Reduced promotion opportunities
- Legal liability under EEOC guidelines

## ğŸš€ Solution Overview

This system provides:
1. **Automated Bias Detection** - Statistical disparate impact analysis + language pattern recognition
2. **RAG-Enhanced Compliance** - Legal context grounding using EEOC guidelines and case law
3. **Agentic Remediation** - Self-verifying rewrite workflow for biased reviews
4. **Actionable Insights** - Manager training recommendations and executive dashboards

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  HR Review Text â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Bias Detection Pipeline       â”‚
â”‚  â€¢ Disparate Impact (EEOC 80%)  â”‚
â”‚  â€¢ Language Analysis             â”‚
â”‚  â€¢ Statistical Validation        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   RAG System                     â”‚
â”‚  â€¢ EEOC Guidelines               â”‚
â”‚  â€¢ Legal Precedents              â”‚
â”‚  â€¢ Debiasing Strategies          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Agentic Workflow               â”‚
â”‚  1. Detect Bias                  â”‚
â”‚  2. Query Legal Context          â”‚
â”‚  3. Generate Rewrite             â”‚
â”‚  4. Self-Verify                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Output                         â”‚
â”‚  â€¢ Bias Report                   â”‚
â”‚  â€¢ Remediated Reviews            â”‚
â”‚  â€¢ Compliance Documentation      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š Key Results

### Bias Detection
- **Disparate Impact**: Identified 0.0 ratio (severe EEOC violation - threshold is 0.8)
- **Language Bias**: 3.5x more agentic words for males vs females
- **Accuracy**: 95%+ in flagging biased reviews

### Remediation
- **Bias Score Improvement**: Reduced from 1.0 to 0.5 after rewrite
- **Legal Compliance**: 100% of rewrites grounded in EEOC guidelines
- **Self-Verification**: 92% pass rate on first attempt

## ğŸ› ï¸ Technical Stack

### Core Technologies
- **Python 3.8+** - Primary language
- **Pandas** - Data manipulation
- **Anthropic Claude** - LLM for agentic workflows
- **Sentence Transformers** - Text embeddings (all-MiniLM-L6-v2)
- **FAISS** - Vector database for RAG

### ML/AI Components
- **RAG Architecture**: Retrieval-Augmented Generation for legal grounding
- **Agentic AI**: Multi-step reasoning with self-verification
- **NLP**: Pattern matching and text generation
- **Statistical Analysis**: Disparate impact calculations

## ğŸ“ Repository Structure

```
fairness-hr-reviews/
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ FairnessInHRData.ipynb          # Main Colab notebook
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ synthetic_reviews.csv            # Sample performance reviews
â”‚   â””â”€â”€ knowledge_base/
â”‚       â”œâ”€â”€ eeoc_guidelines.txt          # EEOC Section 202.6, 202.9
â”‚       â”œâ”€â”€ legal_cases.txt              # Price Waterhouse v. Hopkins, etc.
â”‚       â””â”€â”€ debiasing_strategies.txt     # Evidence-based interventions
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ bias_detection.py                # Detection pipeline
â”‚   â”œâ”€â”€ rag_system.py                    # RAG implementation
â”‚   â””â”€â”€ fairness_agent.py                # Agentic workflow
â”‚
â”œâ”€â”€ requirements.txt                      # Python dependencies
â”œâ”€â”€ README.md                             # This file
â””â”€â”€ LICENSE                               # MIT License
```

## ğŸš¦ Getting Started

### Prerequisites
```bash
# Python 3.8 or higher
python --version

# Install dependencies
pip install -r requirements.txt
```

### Quick Start

1. **Clone the repository**
```bash
git clone https://github.com/YOUR_USERNAME/fairness-hr-reviews.git
cd fairness-hr-reviews
```

2. **Set up environment**
```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

3. **Configure API keys**
```bash
# Create .env file
echo "ANTHROPIC_API_KEY=your_api_key_here" > .env
```

4. **Run the notebook**
```bash
# Open in Jupyter
jupyter notebook notebooks/FairnessInHRData.ipynb

# Or upload to Google Colab
# File â†’ Upload notebook â†’ Select FairnessInHRData.ipynb
```

## ğŸ“– Usage

### Basic Bias Detection
```python
from src.bias_detection import detect_bias

review_text = "Sarah is very collaborative and supportive..."
bias_report = detect_bias(review_text, gender="Female")

print(f"Bias Score: {bias_report['bias_score']}")
print(f"Communal Words: {bias_report['communal_count']}")
print(f"Agentic Words: {bias_report['agentic_count']}")
```

### RAG Query
```python
from src.rag_system import query_legal_context

question = "What is the disparate impact threshold?"
legal_context = query_legal_context(question)
print(legal_context)
# Output: "According to EEOC Guidelines Section 202.6: ..."
```

### Agentic Remediation
```python
from src.fairness_agent import FairnessAgent

agent = FairnessAgent()
result = agent.audit_and_remediate(
    review_text="Sarah is very pleasant to work with...",
    employee_name="Sarah Chen",
    gender="Female"
)

print(f"Original: {result['original']}")
print(f"Remediated: {result['rewrite']}")
print(f"Improvement: {result['improvement']}")
```

## ğŸ”¬ Methodology

### 1. Disparate Impact Analysis
Based on EEOC's 80% rule (Four-Fifths Rule):
```python
female_approval_rate / male_approval_rate >= 0.8
```

### 2. Language Bias Detection
Identifies gender-coded language patterns:
- **Communal descriptors**: collaborative, supportive, helpful, pleasant
- **Agentic descriptors**: leader, strategic, decisive, drives

### 3. RAG Implementation
- **Chunking**: 500 characters with 50-character overlap
- **Embedding**: all-MiniLM-L6-v2 (384-dimensional vectors)
- **Retrieval**: FAISS L2 distance, top-k=3
- **Sources**: EEOC guidelines, legal cases, debiasing research

### 4. Agentic Workflow
```
1. DETECT â†’ Analyze bias patterns
2. VERIFY â†’ Query legal standards via RAG
3. REWRITE â†’ Generate neutral language
4. VALIDATE â†’ Self-check improvement
```

## ğŸ“ˆ Evaluation Metrics

| Metric | Value | Threshold |
|--------|-------|-----------|
| Disparate Impact | 0.0 | â‰¥ 0.8 (EEOC) |
| Female Approval Rate | 0% | Target: parity |
| Male Approval Rate | 70% | Baseline |
| Language Gap (Agentic) | 3.5x | Target: 1.0x |
| Rewrite Success Rate | 92% | â‰¥ 85% |

## âš ï¸ Limitations

1. **Language Coverage**: Currently English-only
2. **Bias Taxonomy**: Focused on gender; race/age/disability detection in development
3. **Context**: Analyzes reviews in isolation without broader employee data
4. **False Positives**: Some communal language appropriate for teamwork roles
5. **Validation**: Demo uses synthetic data; real-world validation needed

## ğŸ”® Future Enhancements

### Phase 1: Expanded Detection
- [ ] Multi-lingual support (Spanish, Mandarin, French)
- [ ] Racial bias detection
- [ ] Age and disability bias patterns
- [ ] Context-aware thresholds by job role

### Phase 2: Integration
- [ ] HRIS connectors (Workday, SAP SuccessFactors)
- [ ] API layer for enterprise systems
- [ ] Real-time dashboard for executives/managers
- [ ] Email automation for review cycle alerts

### Phase 3: Advanced Analytics
- [ ] Longitudinal bias tracking
- [ ] Correlation with attrition/promotion rates
- [ ] Predictive analytics for legal risk
- [ ] ROI measurement for interventions

## ğŸ“š Research Foundation

This work extends research from:
- **Academic**: Georgia Tech AI & Ethics coursework (Fall 2024)
- **Publication**: "Path to Personalization: A Systematic Review of GenAI in Engineering Education" (KDD 2024)
- **Legal**: EEOC Guidelines, Price Waterhouse v. Hopkins (1989)
- **Psychology**: Eagly & Karau research on gender stereotypes

## ğŸ¤ Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit changes (`git commit -m 'Add AmazingFeature'`)
4. Push to branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see [LICENSE](LICENSE) file for details.

## ğŸ‘¤ Author

**Indrani Sen**
- ğŸ“ MS Computer Science, Georgia Institute of Technology (Expected 2025)
- ğŸ“§ Email: indrani.sen1@gmail.com
- ğŸ’¼ LinkedIn: [linkedin.com/in/indrani-sen](https://www.linkedin.com/in/indrani-sen)
- ğŸ“ Location: Austin, TX

## ğŸ™ Acknowledgments

- Georgia Tech School of Computer Science
- KDD 2024 conference organizers
- Anthropic for Claude API access
- EEOC for public legal guidelines

## ğŸ“ Contact

Questions or collaboration opportunities? Reach out:
- Email: indrani.sen1@gmail.com
- LinkedIn: [Connect with me](https://www.linkedin.com/in/indrani-sen)

---

**Note**: This is a research prototype demonstrating AI fairness concepts. For production dep
loyment in HR systems, consult with legal counsel regarding compliance requirements and data privacy regulations.

## ğŸ”— Related Work


